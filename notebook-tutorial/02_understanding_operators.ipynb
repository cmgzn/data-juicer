{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Understanding Data-Juicer Operators\n",
    "\n",
    "In this notebook, we'll explore the different types of operators in Data-Juicer and how to use them. Operators are the building blocks of data processing pipelines in Data-Juicer.\n",
    "\n",
    "## What are Operators?\n",
    "\n",
    "Operators in Data-Juicer are specialized functions that perform specific data processing tasks. Each operator is designed to handle a particular aspect of data cleaning, transformation, or filtering.\n",
    "\n",
    "## Operator Types\n",
    "\n",
    "Data-Juicer provides 6 main types of operators:\n",
    "\n",
    "- **Mapper**: Edits and transforms samples (e.g., text cleaning)\n",
    "- **Filter**: Filters out low-quality samples (e.g., language filtering)\n",
    "- **Deduplicator**: Detects and removes duplicate samples\n",
    "- **Selector**: Selects top samples based on ranking\n",
    "- **Grouper**: Group samples to batched samples\n",
    "- **Aggregator**: Aggregate for batched samples, such as summary or conclusion\n",
    "\n",
    "In this notebook, we'll explore each type with practical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary modules and create some sample data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_juicer.core.data import NestedDataset as Dataset\n",
    "\n",
    "# Create sample data\n",
    "sample_data = [\n",
    "    {\"text\": \"Hello world! This is a sample text with good quality.Visit https://example.com for more info. Email me at test@example.com\"},\n",
    "    {\"text\": \"This text has many repeated words words words words words words words words words words words words\"},\n",
    "    {\"text\": \"Short\"},\n",
    "    {\"text\": \"This is a high quality English text with appropriate length and good content.\"},\n",
    "    {\"text\": \"Bonjour le monde! Ceci est un texte d'exemple de bonne qualit√©.\"},\n",
    "    {\"text\": \"hello world! this is a sample text with good quality.\"},\n",
    "    {\"text\": \"This is a high quality English text with appropriate length and good content.\"}\n",
    "]\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_list(sample_data)\n",
    "\n",
    "print(\"Sample dataset created with\", len(sample_data), \"samples\")\n",
    "print(\"\\nOriginal samples:\")\n",
    "for i, sample in enumerate(sample_data):\n",
    "    print(f\"{i+1}. {sample['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mapper-operators",
   "metadata": {},
   "source": [
    "## 1. Mapper Operators\n",
    "\n",
    "Mapper operators transform data samples. They take one sample and return a transformed version of that sample.\n",
    "\n",
    "Let's try the `clean_links_mapper` and `clean_email_mapper`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mapper-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_juicer.ops.mapper import CleanLinksMapper, CleanEmailMapper\n",
    "\n",
    "# Create mapper operators\n",
    "clean_links_op = CleanLinksMapper()\n",
    "clean_email_op = CleanEmailMapper()\n",
    "\n",
    "# You can apply mappers one by one\n",
    "print(\"Original text:\")\n",
    "sample_text = sample_data[0]['text']\n",
    "print(sample_text)\n",
    "\n",
    "# Process with mappers\n",
    "sample = {'text': [sample_text]}  # Note: When using a batched operator, the single-sample format represents text as a list.\n",
    "sample = clean_links_op.process(sample)\n",
    "sample = clean_email_op.process(sample)\n",
    "\n",
    "print(\"\\nAfter processing:\")\n",
    "print(sample['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918dc8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or apply mappers in a pipeline\n",
    "\n",
    "print(\"Original dataset:\")\n",
    "for i, sample in enumerate(dataset):\n",
    "    print(f\"{i+1}. {sample['text']}\")\n",
    "\n",
    "# Apply mappers\n",
    "# You can use `dataset.process` to apply operators in a pipeline\n",
    "# or use `op.run` to apply a single operator\n",
    "# dataset = clean_links_op.run(dataset)\n",
    "# dataset = clean_email_op.run(dataset)\n",
    "dataset = dataset.process([clean_links_op, clean_email_op])\n",
    "\n",
    "print(\"\\nAfter processing:\")\n",
    "for i, sample in enumerate(dataset):\n",
    "    print(f\"{i+1}. {sample['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filter-operators",
   "metadata": {},
   "source": [
    "## 2. Filter Operators\n",
    "\n",
    "Filter operators remove data samples that don't meet certain criteria. They compute statistics and then decide whether to keep or remove a sample.\n",
    "\n",
    "Let's try the `text_length_filter` `alphanumeric_filter` `language_id_score_filter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filter-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_juicer.ops.filter import TextLengthFilter, AlphanumericFilter, LanguageIDScoreFilter\n",
    "\n",
    "# Create filter operators\n",
    "length_filter = TextLengthFilter(min_len=10, max_len=100)\n",
    "alpha_filter = AlphanumericFilter(min_ratio=0.5)\n",
    "lang_filter = LanguageIDScoreFilter(lang='en', min_score=0.8)\n",
    "\n",
    "print(\"Applying filters to dataset:\")\n",
    "print(\"Original dataset size:\", len(dataset))\n",
    "\n",
    "# Apply filters\n",
    "filtered_dataset = length_filter.run(dataset, reduce=False)  # Just compute stats\n",
    "filtered_dataset = alpha_filter.run(filtered_dataset, reduce=False)  # Just compute stats\n",
    "filtered_dataset = lang_filter.run(filtered_dataset, reduce=False)  # Just compute stats\n",
    "\n",
    "# Show stats\n",
    "print(\"\\nDataset with computed stats:\")\n",
    "for i, sample in enumerate(filtered_dataset):\n",
    "    stats = sample.get('__dj__stats__', {})\n",
    "    print(f\"{i+1}. Text: {sample['text']}\")\n",
    "    print(f\"   Stats: Length={stats.get('text_len', 'N/A')}, Alpha ratio={stats.get('alnum_ratio', 'N/A'):.2f}, Language score={stats.get('lang_score', 'N/A'):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ef32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply actual filtering\n",
    "final_dataset = length_filter.run(dataset)  # Compute stats and filter\n",
    "final_dataset = alpha_filter.run(final_dataset)  # Compute stats and filter\n",
    "final_dataset = lang_filter.run(final_dataset)  # Compute stats and filter\n",
    "\n",
    "print(\"\\nAfter filtering:\")\n",
    "print(\"Final dataset size:\", len(final_dataset))\n",
    "for i, sample in enumerate(final_dataset):\n",
    "    print(f\"{i+1}. {sample['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deduplicator-operators",
   "metadata": {},
   "source": [
    "## 3. Deduplicator Operators\n",
    "\n",
    "Deduplicator operators identify and remove duplicate samples from the dataset.\n",
    "\n",
    "Let's try the `document_deduplicator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deduplicator-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_juicer.ops.deduplicator import DocumentDeduplicator\n",
    "\n",
    "# Create deduplicator operator\n",
    "dedup_op = DocumentDeduplicator(lowercase=True)  # Case-insensitive deduplication\n",
    "\n",
    "print(\"Dataset before deduplication:\")\n",
    "for i, sample in enumerate(dataset):\n",
    "    print(f\"{i+1}. {sample['text']}\")\n",
    "\n",
    "# Apply deduplication\n",
    "deduped_dataset = dedup_op.run(dataset)\n",
    "\n",
    "print(\"\\nDataset after deduplication:\")\n",
    "print(\"Size before:\", len(dataset), \"Size after:\", len(deduped_dataset))\n",
    "for i, sample in enumerate(deduped_dataset):\n",
    "    print(f\"{i+1}. {sample['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selector-operators",
   "metadata": {},
   "source": [
    "## 4. Selector Operators\n",
    "\n",
    "Selector operators select a subset of samples based on certain criteria.\n",
    "\n",
    "Let's create a dataset with metadata and use `topk_specified_field_selector`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selector-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_juicer.ops.selector import TopkSpecifiedFieldSelector\n",
    "\n",
    "# Create sample data with metadata\n",
    "sample_data_with_meta = [\n",
    "    {\"text\": \"Sample text 1\", \"meta\": {\"quality_score\": 0.8}},\n",
    "    {\"text\": \"Sample text 2\", \"meta\": {\"quality_score\": 0.9}},\n",
    "    {\"text\": \"Sample text 3\", \"meta\": {\"quality_score\": 0.7}},\n",
    "    {\"text\": \"Sample text 4\", \"meta\": {\"quality_score\": 0.95}},\n",
    "    {\"text\": \"Sample text 5\", \"meta\": {\"quality_score\": 0.6}},\n",
    "]\n",
    "\n",
    "meta_dataset = Dataset.from_list(sample_data_with_meta)\n",
    "\n",
    "print(\"Dataset with quality scores:\")\n",
    "for i, sample in enumerate(meta_dataset):\n",
    "    print(f\"{i+1}. {sample['text']} (Score: {sample['meta']['quality_score']})\")\n",
    "\n",
    "# Create selector operator\n",
    "selector_op = TopkSpecifiedFieldSelector(field_key='meta.quality_score', topk=3)\n",
    "\n",
    "# Apply selection\n",
    "selected_dataset = selector_op.process(meta_dataset)\n",
    "\n",
    "print(\"\\nTop 3 samples by quality score:\")\n",
    "for i, sample in enumerate(selected_dataset):\n",
    "    print(f\"{i+1}. {sample['text']} (Score: {sample['meta']['quality_score']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-ways",
   "metadata": {},
   "source": [
    "## Different Ways to Use Operators\n",
    "\n",
    "There are three main ways to use operators in Data-Juicer:\n",
    "\n",
    "1. **Direct processing**: `op.process(sample)` - For single sample processing\n",
    "2. **Functional programming style**: `op.run(dataset)` - For turnkey pipeline processing\n",
    "3. **Chain call style**: `dataset.process([op1, op2, op3])` - For batch processing with automatic control\n",
    "\n",
    "Let's see these in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-ways-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Direct processing\n",
    "clean_links_op = CleanLinksMapper()\n",
    "sample = {'text': [\"Visit https://example.com\"]}\n",
    "result = clean_links_op.process(sample)\n",
    "print(\"1. Direct processing:\")\n",
    "print(\"   Original:\", sample['text'][0])\n",
    "print(\"   Processed:\", result['text'][0])\n",
    "\n",
    "# 2. Functional programming style\n",
    "small_dataset = Dataset.from_list([{\"text\": \"Visit https://example.com\"}, {\"text\": \"test\"}])\n",
    "result_dataset2 = clean_links_op.run(small_dataset)\n",
    "print(\"\\n2. Functional programming style:\")\n",
    "for i, sample in enumerate(result_dataset2):\n",
    "    print(f\"   {i+1}. {sample['text']}\")\n",
    "\n",
    "# 3. Chain call style\n",
    "text_length_filter = TextLengthFilter(min_len=10)\n",
    "result_dataset3 = small_dataset.process([text_length_filter, clean_links_op])\n",
    "print(\"\\n3. Chain call style:\")\n",
    "for i, sample in enumerate(result_dataset3):\n",
    "    print(f\"   {i+1}. {sample['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "For more operators and their usage, please refer to the [Operator Schemas](https://modelscope.github.io/data-juicer/en/main/docs/Operators.html).\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue with the next notebook to learn how to build data recipes that combine multiple operators into processing pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-juicer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
