{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "fe70e8d4",
            "metadata": {},
            "source": [
                "# Running and Optimizing Data Processing Pipelines"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2040af4c",
            "metadata": {},
            "source": [
                "## What are Data Processing Pipelines?\n",
                "\n",
                "A data processing pipeline in Data-Juicer is a workflow that executes a series of data processing operations. These pipelines are defined via configuration files (recipes) that specify the operators to run and their execution order. The pipeline processes datasets sequentially, resulting in a cleaned and filtered high-quality dataset.\n",
                "\n",
                "## In This Notebook\n",
                "\n",
                "1. Different ways to run data processing pipelines\n",
                "2. Understanding the executor process\n",
                "3. Performance optimization techniques with practical examples"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "26b7b73c",
            "metadata": {},
            "source": [
                "## Methods to Run Data Processing Pipelines\n",
                "\n",
                "Data-Juicer provides multiple ways to run data processing pipelines, including command-line tools and programming interfaces.\n",
                "\n",
                "First of all, copy the demo configuration file to the local configs directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3cab4216",
            "metadata": {},
            "outputs": [],
            "source": [
                "# copy the demo configuration file to local configs directory\n",
                "%mkdir -p configs/demo\n",
                "%mkdir -p demos/data\n",
                "%cp ../configs/demo/process.yaml configs/demo/\n",
                "%cp ../demos/data/demo-dataset.jsonl demos/data/"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "92890dbd",
            "metadata": {},
            "source": [
                "### 1. Running via Command Line\n",
                "\n",
                "This is the most common approach. You can directly execute the configured processing pipeline using the command-line tool.\n",
                "\n",
                "The [configs/demo/process.yaml](https://github.com/modelscope/data-juicer/blob/main/configs/demo/process.yaml) here is the given data_recipes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "92837f2e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Using the recommended dj-process command line tool\n",
                "!dj-process --config configs/demo/process.yaml\n",
                "\n",
                "# Or, if installed from source, using Python script\n",
                "# !python tools/process_data.py --config configs/demo/process.yaml"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4783a35b",
            "metadata": {},
            "source": [
                "The [process_data.py](https://github.com/modelscope/data-juicer/blob/main/tools/process_data.py) will call the executor.run() method to process the data, Support RAY mode."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9ceca6ab",
            "metadata": {},
            "source": [
                "### 2. Running Programmatically\n",
                "\n",
                "Besides command-line tools, you can also run data processing pipeline directly in Python code.\n",
                "\n",
                "Let's see how to run a pipeline programmatically:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "094baa05",
            "metadata": {},
            "outputs": [],
            "source": [
                "# we init the corresponding config\n",
                "from data_juicer.config import init_configs\n",
                "cfg = init_configs(['--config', 'configs/demo/process.yaml'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c9fd5189",
            "metadata": {},
            "outputs": [],
            "source": [
                "from data_juicer.core import DefaultExecutor\n",
                "\n",
                "# Create and run executor\n",
                "executor = DefaultExecutor(cfg)\n",
                "dataset = executor.run()\n",
                "\n",
                "print(\"\\nProcessed samples:\")\n",
                "for i, sample in enumerate(dataset):\n",
                "    print(f\"{i+1}. {sample['text']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "019fbcc8",
            "metadata": {},
            "source": [
                "## Understanding the Executor Process\n",
                "\n",
                "Let's break down what happens when we run the executor. The DefaultExecutor.run() method in [default_executor.py](https://github.com/modelscope/data-juicer/blob/main/data_juicer/core/executor/default_executor.py) performs several key steps:\n",
                "\n",
                "### 1. Loading and Formatting Data\n",
                "\n",
                "First, the method loads and formats the data. It can load dataset from a checkpoint in previous run, or load dataset from the data formatter."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "83be2e80",
            "metadata": {},
            "source": [
                "``` python \n",
                "    def run(\n",
                "        self,\n",
                "        dataset: Union[Dataset, NestedDataset] = None,\n",
                "        load_data_np: Optional[PositiveInt] = None,\n",
                "        skip_export: bool = False,\n",
                "        skip_return: bool = False,\n",
                "    ):\n",
                "        ...\n",
                "        # 1. format data\n",
                "        if dataset is not None:\n",
                "            logger.info(f\"Using existing dataset {dataset}\")\n",
                "        elif self.cfg.use_checkpoint and self.ckpt_manager.ckpt_available:\n",
                "            logger.info(\"Loading dataset from checkpoint...\")\n",
                "            dataset = self.ckpt_manager.load_ckpt()\n",
                "        else:\n",
                "            logger.info(\"Loading dataset from dataset builder...\")\n",
                "            if load_data_np is None:\n",
                "                load_data_np = self.np\n",
                "            dataset = self.dataset_builder.load_dataset(num_proc=load_data_np)\n",
                "        ...\n",
                "```\n",
                "\n",
                "You can run the code below to see the dataset here interactively."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b8d354b6",
            "metadata": {},
            "outputs": [],
            "source": [
                "loaded_dataset = executor.dataset_builder.load_dataset(num_proc=cfg.np)\n",
                "for sample in loaded_dataset:\n",
                "    print(sample)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "59da0d88",
            "metadata": {},
            "source": [
                "### 2. Loading Operators\n",
                "\n",
                "Next, the method loads the operators from the given configuration file.\n",
                "\n",
                "``` python\n",
                "\n",
                "    def run(self, load_data_np = None)\n",
                "        ...\n",
                "        # 2. extract processes\n",
                "        logger.info(\"Preparing process operators...\")\n",
                "        ops = load_ops(self.cfg.process)\n",
                "        ...\n",
                "\n",
                "```\n",
                "\n",
                "You can run the code below to see the ops."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "942fbab2",
            "metadata": {},
            "outputs": [],
            "source": [
                "from data_juicer.ops import load_ops\n",
                "ops = load_ops(cfg.process)\n",
                "for op in ops:\n",
                "    print(op.__class__.__name__)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "267566cc",
            "metadata": {},
            "source": [
                "### 3. Processing Data\n",
                "\n",
                "Then, the method applies the loaded operators to the data. All data samples are processed through the list of operators.\n",
                "\n",
                "``` python\n",
                "    def run():\n",
                "        ...\n",
                "        # 3. data process\n",
                "        # - If tracer is open, trace each op after it's processed\n",
                "        # - If checkpoint is open, clean the cache files after each process\n",
                "        logger.info(\"Processing data...\")\n",
                "        tstart = time()\n",
                "        dataset = dataset.process(\n",
                "            ops,\n",
                "            work_dir=self.work_dir,\n",
                "            exporter=self.exporter,\n",
                "            checkpointer=self.ckpt_manager,\n",
                "            tracer=self.tracer,\n",
                "            adapter=self.adapter,\n",
                "            open_monitor=self.cfg.open_monitor,\n",
                "        )\n",
                "        tend = time()\n",
                "        logger.info(f\"All OPs are done in {tend - tstart:.3f}s.\")\n",
                "        ...\n",
                "```\n",
                "\n",
                "Here, all data samples are processed through the list of ops."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b1da3efb",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = loaded_dataset.process(\n",
                "    ops,\n",
                "    work_dir=cfg.work_dir,\n",
                "    exporter=executor.exporter,\n",
                "    checkpointer=executor.ckpt_manager,\n",
                "    tracer=executor.tracer,\n",
                "    adapter=executor.adapter,\n",
                "    open_monitor=cfg.open_monitor\n",
                ")\n",
                "\n",
                "for sample in dataset:\n",
                "    print(sample)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "af70bf23",
            "metadata": {},
            "source": [
                "### 4. Exporting Data\n",
                "\n",
                "Finally, the method exports the processed dataset to the specified path.\n",
                "\n",
                "``` python\n",
                "    def run():\n",
                "        ...\n",
                "        # 4. data export\n",
                "        if not skip_export:\n",
                "            logger.info('Exporting dataset to disk...')\n",
                "            self.exporter.export(dataset)\n",
                "        ...\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9ce13e80",
            "metadata": {},
            "source": [
                "You can check the processed dataset and statistics in the export path specified in [configs/demo/process.yaml](https://github.com/modelscope/data-juicer/blob/main/configs/demo/process.yaml)\n",
                "\n",
                "``` yaml\n",
                "export_path: './outputs/demo-process/demo-processed.jsonl'\n",
                "```\n",
                "\n",
                "After processing, Data-Juicer generates two main output files:\n",
                "- **demo-processed.jsonl**: The processed dataset\n",
                "- **demo-processed_stats.jsonl**: Processing statistics and metadata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "analyze-sample-quality",
            "metadata": {},
            "outputs": [],
            "source": [
                "cfg.export_path"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ae6ab7ec",
            "metadata": {},
            "source": [
                "## Performance Optimization Tips"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f24d6bf7",
            "metadata": {},
            "outputs": [],
            "source": [
                "import tempfile\n",
                "\n",
                "# Create sample data with repetitive patterns\n",
                "sample_data = []\n",
                "for i in range(500):\n",
                "    # Some samples with repeated words to test word repetition filter\n",
                "    if i % 3 == 0:\n",
                "        sample_data.append({\"text\": \"word word word word word word word word word word \" * 5})\n",
                "    elif i % 3 == 1:\n",
                "        sample_data.append({\"text\": f\"This is a high quality English text with appropriate length and good content for sample {i}.\"})\n",
                "    else:\n",
                "        sample_data.append({\"text\": f\"short text but pneumonoultramicroscopicsilicovolcanoconiosis\"})\n",
                "\n",
                "# Write to temporary file\n",
                "with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.jsonl') as f:\n",
                "    import json\n",
                "    for item in sample_data:\n",
                "        f.write(json.dumps(item) + '\\n')\n",
                "    temp_data_path = f.name"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "25d7b4d9",
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "from data_juicer.config import init_configs\n",
                "from data_juicer.core import DefaultExecutor\n",
                "\n",
                "# Recipe with parallel processing\n",
                "parallel_recipe = f\"\"\"\n",
                "project_name: 'np_example'\n",
                "dataset_path: '{temp_data_path}'\n",
                "export_path: './outputs/np_dataset.jsonl'\n",
                "np: 4\n",
                "\n",
                "process:\n",
                "  - language_id_score_filter:\n",
                "      lang: 'en'\n",
                "      min_score: 0.8\n",
                "  - words_num_filter:\n",
                "      lang: 'en'\n",
                "      min_num: 5\n",
                "      max_num: 1000\n",
                "  - word_repetition_filter:\n",
                "      lang: 'en'\n",
                "      max_ratio: 0.5\n",
                "      rep_len: 5\n",
                "  - stopwords_filter:\n",
                "      lang: 'en'\n",
                "      min_ratio: 0.1\n",
                "\"\"\"\n",
                "\n",
                "# Write recipe to file\n",
                "with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n",
                "    f.write(parallel_recipe)\n",
                "    f.flush()\n",
                "    parallel_config_path = f.name\n",
                "\n",
                "print(\"Running with 4 parallel processes...\")\n",
                "start_time = time.time()\n",
                "cfg = init_configs(['--config', parallel_config_path], load_configs_only=True)\n",
                "executor = DefaultExecutor(cfg)\n",
                "parallel_dataset = executor.run()\n",
                "parallel_time = time.time() - start_time\n",
                "print(f\"Parallel processing completed in {parallel_time:.2f} seconds\")\n",
                "print(f\"Processed {len(dataset)} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "operator-fusion-section",
            "metadata": {},
            "source": [
                "### 2. Operator Fusion for Reduced Memory Usage\n",
                "\n",
                "Operator fusion is a powerful optimization technique that combines multiple operators into a single processing step, reducing intermediate data storage and memory usage. This is especially beneficial when operators share common computations. Let's compare processing with and without operator fusion:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dda4163a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Recipe with operator fusion\n",
                "fusion_recipe = f\"\"\"\n",
                "project_name: 'fusion_example'\n",
                "dataset_path: '{temp_data_path}'\n",
                "export_path: './outputs/fusion_dataset.jsonl'\n",
                "np: 4\n",
                "\n",
                "process:\n",
                "  - general_fused_op:\n",
                "      fused_op_list:\n",
                "        - language_id_score_filter:\n",
                "            lang: 'en'\n",
                "            min_score: 0.8\n",
                "        - words_num_filter:\n",
                "            lang: 'en'\n",
                "            min_num: 5\n",
                "            max_num: 1000\n",
                "        - word_repetition_filter:\n",
                "            lang: 'en'\n",
                "            max_ratio: 0.5\n",
                "            rep_len: 5\n",
                "        - stopwords_filter:\n",
                "            lang: 'en'\n",
                "            min_ratio: 0.1\n",
                "\"\"\"\n",
                "\n",
                "\n",
                "# Run with fusion\n",
                "with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n",
                "    f.write(fusion_recipe)\n",
                "    f.flush()\n",
                "    fusion_config_path = f.name\n",
                "\n",
                "print(\"Running with operator fusion...\")\n",
                "start_time = time.time()\n",
                "cfg = init_configs(['--config', fusion_config_path], load_configs_only=True)\n",
                "executor = DefaultExecutor(cfg)\n",
                "dataset_fusion = executor.run()\n",
                "fusion_time = time.time() - start_time"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "621bdf77",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Performance Comparison:\")\n",
                "print(f\"Individual operators: {parallel_time:.2f} seconds ({len(parallel_dataset)} samples)\")\n",
                "print(f\"Fused operator:       {fusion_time:.2f} seconds ({len(dataset_fusion)} samples)\")\n",
                "print(f\"Performance improvement: {((parallel_time - fusion_time) / parallel_time * 100):.1f}%\")\n",
                "\n",
                "assert len(dataset_fusion) == len(parallel_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "other-optimizations",
            "metadata": {},
            "source": [
                "### 3. Additional Optimization Techniques\n",
                "\n",
                "Data-Juicer offers several other optimization techniques that can be combined for maximum performance:\n",
                "\n",
                "#### Caching and Checkpointing for Fault Tolerance\n",
                "\n",
                "For long-running pipelines, enabling caching and checkpointing can save significant time if the process is interrupted:\n",
                "\n",
                "```yaml\n",
                "project_name: 'robust_processing_example'\n",
                "dataset_path: './data/large_dataset.jsonl'\n",
                "export_path: './outputs/processed_dataset.jsonl'\n",
                "\n",
                "# Option 1: Use checkpointing (cache will be disabled automatically)\n",
                "use_checkpoint: true\n",
                "temp_dir: './temp_checkpoints'\n",
                "\n",
                "# Option 2: Use caching (uncomment below and set use_checkpoint to false)\n",
                "# use_cache: true\n",
                "# ds_cache_dir: './cache'\n",
                "\n",
                "process:\n",
                "  - document_deduplicator:\n",
                "      lowercase: true\n",
                "  - text_length_filter:\n",
                "      min_len: 10\n",
                "      max_len: 10000\n",
                "```\n",
                "\n",
                "#### Memory-Efficient Processing with Compressed Caching\n",
                "\n",
                "When working with limited disk space, you can enable compressed caching to reduce storage footprint:\n",
                "\n",
                "```yaml\n",
                "project_name: 'memory_efficient_example'\n",
                "dataset_path: './data/large_dataset.jsonl'\n",
                "export_path: './outputs/processed_dataset.jsonl'\n",
                "\n",
                "# Use compressed caching to reduce memory usage\n",
                "cache_compress: 'gzip'\n",
                "\n",
                "process:\n",
                "  - clean_html_mapper: {}\n",
                "  - language_id_score_filter:\n",
                "      lang: 'en'\n",
                "      min_score: 0.8\n",
                "```\n",
                "\n",
                "#### Large-Scale Data Sharding\n",
                "\n",
                "For extremely large datasets, you can shard the export to manage file sizes and facilitate parallel processing workflows:\n",
                "\n",
                "```yaml\n",
                "project_name: 'sharded_export_example'\n",
                "dataset_path: './data/very_large_dataset.jsonl'\n",
                "export_path: './outputs/sharded_dataset.jsonl'\n",
                "\n",
                "# Shard the output into files with maximum 10000 samples each\n",
                "# Parallel export across shards happens automatically\n",
                "export_shard_size: 10000\n",
                "\n",
                "process:\n",
                "  - text_length_filter:\n",
                "      min_len: 10\n",
                "      max_len: 10000\n",
                "```\n",
                "\n",
                "These optimization techniques can significantly improve the performance of your Data-Juicer pipelines. For detailed configuration options and advanced settings, please refer to [config_all.yaml](https://github.com/modelscope/data-juicer/blob/main/configs/config_all.yaml)."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "54788141",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "Continue with the next notebook in the series to learn about data quality analysis and visualization with Data-Juicer's Analyzer module, which helps you understand dataset characteristics and make informed processing decisions."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "data-juicer",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
