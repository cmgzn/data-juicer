{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e3ddc2c-7871-47ee-ad42-02676bf0451e",
   "metadata": {},
   "source": [
    "# Develop a New Operator\n",
    "\n",
    "This tutorial shows you how to create custom operators for Data-Juicer. We'll cover the complete process from design to testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5e5e7f",
   "metadata": {},
   "source": [
    "## Understanding Operator Types\n",
    "\n",
    "Data-Juicer supports several types of operators:\n",
    "\n",
    "1. **Mapper**: Transforms data samples\n",
    "2. **Filter**: Removes data samples based on criteria\n",
    "3. **Deduplicator**: Removes duplicate data samples\n",
    "4. **Selector**: Selects a subset of data samples\n",
    "5. **Grouper**: Groups data samples\n",
    "6. **Aggregator**: Combines multiple data samples\n",
    "\n",
    "Each operator type has its own base class in `data_juicer/ops/base_op.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding for Your Operator\n",
    "\n",
    "Before implementing a new operator, please refer to existing [Operators Zoo](../docs/Operators.md) to avoid unnecessary duplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a New Operator File\n",
    "\n",
    "Create a new operator file in the appropriate directory under `data_juicer/ops/`. For example, for a filter operator, create the file in `data_juicer/ops/filter/`.\n",
    "\n",
    "Let's implement a `YourTextLengthFilter` as an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449de5aa",
   "metadata": {},
   "source": [
    "(Optional) If the new OP defines some statistical variables, please add the corresponding new `StatsKeysConstant` attribute in `data_juicer/utils/constant.py` for unified management.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatsKeysConstant(object):\n",
    "    # ... other keys\n",
    "    text_len = 'text_len'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Dict, Any\n",
    "from jsonargparse.typing import PositiveInt\n",
    "\n",
    "from data_juicer.utils.constant import Fields\n",
    "# NOTE: use a new definition above\n",
    "from data_juicer.utils.constant import StatsKeys\n",
    "from data_juicer.ops.base_op import OPERATORS, Filter\n",
    "\n",
    "@OPERATORS.register_module('your_text_length_filter')\n",
    "class YourTextLengthFilter(Filter):\n",
    "    \"\"\"Filter to keep samples with total text length within a specific\n",
    "    range. \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 min_len: PositiveInt = 10,\n",
    "                 max_len: PositiveInt = sys.maxsize,\n",
    "                 *args,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the enhanced text length filter.\n",
    "\n",
    "        :param min_len: Minimum text length threshold\n",
    "        :param max_len: Maximum text length threshold  \n",
    "        :param args: Additional arguments\n",
    "        :param kwargs: Additional keyword arguments\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.min_len = min_len\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def compute_stats_single(self, sample: Dict[str, Any], context: bool = False) -> Dict[str, Any]:\n",
    "        # Check if already computed\n",
    "        if StatsKeys.text_len in sample[Fields.stats]:\n",
    "            return sample\n",
    "\n",
    "        # Store the computed statistic\n",
    "        sample[Fields.stats][StatsKeys.text_len] = len(sample[self.text_key])\n",
    "        return sample\n",
    "\n",
    "    def process_single(self, sample: Dict[str, Any]) -> bool:\n",
    "        length = sample[Fields.stats][StatsKeys.text_len]\n",
    "        return self.get_keep_boolean(length, self.min_len, self.max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b661bf93",
   "metadata": {},
   "source": [
    "### Key Implementation Details\n",
    "\n",
    "1. **Registration**: Use `@OPERATORS.register_module(\"operator_name\")` to register your operator.\n",
    "\n",
    "2. **Inheritance**: Inherit from the appropriate base class (`Filter`, `Mapper`, etc.) from `data_juicer.ops.base_op`.\n",
    "\n",
    "3. **Single Processing**: Implement `compute_stats_single` and `process_single` methods.\n",
    "\n",
    "4. **Stats Computation**: Use `Fields.stats` and `StatsKeys` from `data_juicer.utils.constant` to store and access statistics.\n",
    "\n",
    "5. **Configuration Parameters**: Accept parameters in `__init__` method for customization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Add to Operator Imports\n",
    "\n",
    "Add your new operator to the corresponding `__init__.py` file in the operator directory. For example, for a filter operator, add it to `data_juicer/ops/filter/__init__.py`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# other OPs\n",
    "from .your_text_length_filter import YourTextLengthFilter  # import this new OP class\n",
    "__all__ = [\n",
    "    # other Ops\n",
    "    \"YourTextLengthFilter\",  # add this new Op to __all__\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for Your Operator\n",
    "\n",
    "It's better to add corresponding tests for your own OPs. For `YourTextLengthFilter` above, you would like to add `test_text_length_filter.py` into `tests/ops/filter/` directory as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "from data_juicer.core.data import NestedDataset as Dataset\n",
    "\n",
    "# NOTE: use a new definition above\n",
    "# from data_juicer.ops.filter.text_length_filter import YourTextLengthFilter\n",
    "from data_juicer.utils.unittest_utils import DataJuicerTestCaseBase\n",
    "\n",
    "class YourTextLengthFilterTest(DataJuicerTestCaseBase):\n",
    "\n",
    "    def _run_text_length_filter(self, dataset: Dataset, target_list, op):\n",
    "        dataset = op.run(dataset)\n",
    "        res_list = dataset.to_list()\n",
    "        print(res_list)\n",
    "        self.assertEqual(res_list, target_list)\n",
    "\n",
    "    def test_case1(self):\n",
    "\n",
    "        ds_list = [{\n",
    "            'text': '123'\n",
    "        }, {\n",
    "            'text': '12345'\n",
    "        }, {\n",
    "            'text': '1234567'\n",
    "        }]\n",
    "        tgt_list = [{\n",
    "            'text': '12345'\n",
    "        }]\n",
    "        dataset = Dataset.from_list(ds_list)\n",
    "        op = YourTextLengthFilter(min_len=4, max_len=6)\n",
    "        self._run_text_length_filter(dataset, tgt_list, op)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # unittest.main()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Features\n",
    "\n",
    "### CUDA Support\n",
    "\n",
    "If your operator uses models that can be accelerated with CUDA, you can enable GPU support:\n",
    "\n",
    "```python\n",
    "    # ... (same as above)\n",
    "    from data_juicer.utils.model_utils import get_model, prepare_model\n",
    "\n",
    "    @OPERATORS.register_module('your_text_length_filter_with_cuda')\n",
    "    class YourTextLengthFilterWithCuda(Filter):\n",
    "        _accelerator = 'cuda' # Enable CUDA acceleration\n",
    "        def __init__(self,\n",
    "                    hf_model: str = 'bert-base-uncased',\n",
    "                    min_len: PositiveInt = 10,\n",
    "                    max_len: PositiveInt = sys.maxsize,\n",
    "                    *args,\n",
    "                    **kwargs):\n",
    "            # ... (same as above)\n",
    "            self.model_key = prepare_model(\n",
    "                model_type=\"huggingface\", pretrained_model_name_or_path=hf_model\n",
    "            )\n",
    "\n",
    "        def compute_stats_single(self, sample, rank=None, context=False):\n",
    "            # ... (some codes)\n",
    "            model, _ = get_model(self.model_key, rank, self.use_cuda())\n",
    "            \n",
    "        def process_single(self, sample=None, rank=None):\n",
    "            # ... (same as above)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Support\n",
    "\n",
    "If an operator takes multiple samples as input and produces multiple samples, the input and output need to be batched together by declaring `_batched_op = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from data_juicer.ops.base_op import OPERATORS, Mapper\n",
    "\n",
    "@OPERATORS.register_module('your_batch_mapper')\n",
    "class YourBatchMapper(Mapper):\n",
    "    \"\"\"A mapper operator processing batched samples.\"\"\"\n",
    "\n",
    "    _batched_op = True\n",
    "    \n",
    "    def __init__(self,\n",
    "                *args,\n",
    "                **kwargs):\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def process_batched(self, samples):\n",
    "        for idx, text in enumerate(samples[self.text_key]):\n",
    "            samples[self.text_key][idx] = text + f\": {len(text)}\"\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_juicer.core.data import NestedDataset\n",
    "\n",
    "ds_list = [{\n",
    "            'text': '123'\n",
    "        }, {\n",
    "            'text': '12345'\n",
    "        }, {\n",
    "            'text': '1234567'\n",
    "        }]\n",
    "print('unbatched samples', ds_list)\n",
    "dataset = NestedDataset.from_list(ds_list)\n",
    "op = YourBatchMapper()\n",
    "dataset = op.run(dataset)\n",
    "print(dataset.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Filename\n",
    "\n",
    "Call `transfer_filename` and `add_suffix_to_filename` to get unique paths for saving of extra datas, such as images and videos, to prevent data coverage and ensure process security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_juicer.utils.file_utils import add_suffix_to_filename, transfer_filename\n",
    "from data_juicer.ops.op_fusion import LOADED_VIDEOS\n",
    "from data_juicer.ops.base_op import OPERATORS, Mapper\n",
    "# ... (import some other libraries)\n",
    "\n",
    "OP_NAME = 'your_video_split_by_key_frame_mapper'\n",
    "@OPERATORS.register_module(OP_NAME)\n",
    "@LOADED_VIDEOS.register_module(OP_NAME)\n",
    "class YourVideoSplitByKeyFrameMapper(Mapper):\n",
    "    _batched_op = True\n",
    "    \n",
    "    def __init__(self,\n",
    "             # ... (OP parameters)\n",
    "             split_num = 1,\n",
    "             *args,\n",
    "             **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._init_parameters = self.remove_extra_parameters(locals())\n",
    "        print(f'init parameters: {self._init_parameters}')\n",
    "        self.split_num = split_num\n",
    "\n",
    "    def process_batched(self, sample):\n",
    "        # ... (some codes)\n",
    "        original_video_path = sample['videos'][0]\n",
    "        base_video_path = transfer_filename(\n",
    "                    original_video_path, OP_NAME, **self._init_parameters)\n",
    "        print(f'base path: {base_video_path}')\n",
    "        for count in range(self.split_num):\n",
    "            split_video_path = add_suffix_to_filename(base_video_path,  f'_{count}')\n",
    "            print(f'split {count} path: {split_video_path}')\n",
    "        # ... (some codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {'videos': ['./video.mp4']}\n",
    "print('------ 2 splits ------')\n",
    "op = YourVideoSplitByKeyFrameMapper(split_num=2)\n",
    "op.process(sample)\n",
    "print('------ 3 splits ------')\n",
    "op = YourVideoSplitByKeyFrameMapper(split_num=3)\n",
    "op.process(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish the Documents\n",
    "\n",
    " In order to facilitate the use of other users, we also need to update this new operator information to the corresponding documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `configs/config_all.yaml`: this complete config file contains a list of all OPs and their arguments, serving as an\n",
    "   important document for users to refer to all available OPs. Therefore, after adding the new OP, we need to add it to the process\n",
    "   list (grouped by the OP type and sorted in alphabetical order):\n",
    "   \n",
    "   ```yaml\n",
    "   ...\n",
    "   - your_text_length_filter:                                # filter text with length out of specific range\n",
    "       min_len: 10                                             # the min length of filter range\n",
    "       max_len: 10000                                          # the max length of filter range\n",
    "   ...\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `docs/Operators.md`: this doc maintains categorized lists of available OPs. it is automatically generated and kept up-to-date via a pre-commit hook, so manual edits to the operator tables are usually unnecessary. However, please manually update the \"ref\" column for your operator to include references (e.g., papers, links) or contributor information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Style\n",
    "\n",
    "We define our styles in `.pre-commit-config.yaml`. Before committing,\n",
    "please install `pre-commit` tool to check and modify accordingly:\n",
    "\n",
    "```shell\n",
    "# ===========install pre-commit tool===========\n",
    "pip install pre-commit\n",
    "\n",
    "cd <path_to_data_juicer>\n",
    "# install pre-commit script for data_juicer\n",
    "pre-commit install\n",
    "\n",
    "\n",
    "# ===========check all files===========\n",
    "git add .\n",
    "pre-commit run --all-files\n",
    "\n",
    "# commit after all checking are passed\n",
    "git commit -m \"xxxx\"\n",
    "```\n",
    "\n",
    "**Note**: We have configured pre-commit checks in github workflow. If this \n",
    "check in your PR fails, please locally ① ensure that the relevant \n",
    "dependencies of pre-commit are consistent with the project configuration \n",
    "(which can be completed through `pre-commit clean` and `pre-commit install`); \n",
    "and ② execute `pre-commit run --all-files` before push.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Make your OP fusible\n",
    "\n",
    "- If the calculation process of some intermediate variables in the new OP is reused in other existing OPs, this new OP can be\n",
    "added to the fusible OPs to accelerate the whole data processing with OP fusion technology. (e.g. both the `word_num_filter`\n",
    "and `word_repetition_filter` need to split the input text into words)\n",
    "- When opening OP fusion, these reused calculation processes and intermediate variables can be shared in the `context` between\n",
    "OPs, thus reducing repeated calculations.\n",
    "- OPs that contain common intermediate variables can be fused in OP fusion through the following steps:\n",
    "\n",
    "1. (Optional) If a new intermediate variable is generated in the new OP, we need to add this new intermediate variable name to \n",
    "the `InterVars` class in `utils/constant.py`. In general, we need to add a prefix `DEFAULT_PREFIX` before the name.\n",
    "\n",
    "```python\n",
    "    class InterVars(object):\n",
    "        # text\n",
    "        lines = DEFAULT_PREFIX + 'lines'\n",
    "        words = DEFAULT_PREFIX + 'words'  # add the new intermediate variable here\n",
    "        ...\n",
    "```\n",
    "\n",
    "2. (Optional) We need to define a registry group in `ops/op_fusion.py` for the new intermediate variable in the 1st step, and add\n",
    "this registry group to the registry group list that stores all groups of intermediate variables. This facilitates the OP Fusion module\n",
    "to track OPs involving these intermediate variables.\n",
    "\n",
    "```python\n",
    "    ...\n",
    "    # Type of intermediate vars\n",
    "    # text\n",
    "    INTER_LINES = Registry(InterVars.lines)\n",
    "    INTER_WORDS = Registry(InterVars.words)  # define registry group for the new intermediate variable\n",
    "\n",
    "    # images\n",
    "    LOADED_IMAGES = Registry(InterVars.loaded_images)\n",
    "\n",
    "    # all\n",
    "    ALL_INTER_VARS = [INTER_LINES, INTER_WORDS, LOADED_IMAGES]  # and add it to the registry group list\n",
    "    ...\n",
    "```\n",
    "\n",
    "3. Before the OP class definition that involves the intermediate variable, register this OP in the registry group corresponding\n",
    "to this intermediate variable, indicating that the intermediate variable may be calculated and used in this OP.\n",
    "\n",
    "```python\n",
    "    ...\n",
    "    @OPERATORS.register_module(OP_NAME)\n",
    "    @INTER_WORDS.register_module(OP_NAME)  # register this new OP into the registry group\n",
    "    class WordNumFilter(Filter):\n",
    "    ...\n",
    "```\n",
    "\n",
    "4. In the calculation process of this intermediate variable of the new OP, we can modify the calculation logic to:\n",
    "   1. If the argument `context` is True, it means the OP fusion is opening, so we get the value of this intermediate variable \n",
    "   from `context` first, which has been calculated by the previous OPs.\n",
    "   2. If this intermediate variable doesn't exist in the `context`, it means it's the first time to calculate this variable in this\n",
    "   OP, so we need to define a unique key and use it to store the intermediate variable in the `context` for subsequent OPs after\n",
    "   it's calculated by this new OP.\n",
    "   3. If the argument `context` is False, just follow the normal calculation process.\n",
    "\n",
    "```python\n",
    "    # before modification\n",
    "    ...\n",
    "    tokenizer = get_model(self.model_key)\n",
    "    words = get_words_from_document(\n",
    "        sample[self.text_key],\n",
    "        token_func=tokenizer.encode_as_pieces if tokenizer else None)\n",
    "    ...        \n",
    "\n",
    "    # after modification\n",
    "    ...\n",
    "    words_key = f'{InterVars.words}-{self.model_key}'\n",
    "    if context and words_key in sample[Fields.context]:\n",
    "        # get the value of intermediate variable from context directly\n",
    "        words = sample[Fields.context][words_key]\n",
    "    else:\n",
    "        # normal calculation process\n",
    "        tokenizer = get_model(self.model_key)\n",
    "        words = get_words_from_document(\n",
    "            sample[self.text_key],\n",
    "            token_func=tokenizer.encode_as_pieces if tokenizer else None)\n",
    "        if context:\n",
    "            # After calculating the intermediate variable for the first time,\n",
    "            # store it in the context for subsequent OPs.\n",
    "            sample[Fields.context][words_key] = words\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72b61fa",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Continue with the next notebook to explore real-world applications and case studies of Data-Juicer.\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Operator Development Guide](https://modelscope.github.io/data-juicer/en/main/docs/DeveloperGuide.html)\n",
    "- [Existing Operators](https://github.com/modelscope/data-juicer/tree/main/data_juicer/ops)\n",
    "- [Test Examples](https://github.com/modelscope/data-juicer/tree/main/tests/ops)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
