{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Distributed Processing with Data-Juicer\n",
    "\n",
    "In this notebook, we'll explore how to use Data-Juicer's distributed processing capabilities to handle large-scale datasets efficiently. Data-Juicer provides powerful distributed processing features based on the Ray framework.\n",
    "\n",
    "## In this Notebook\n",
    "\n",
    "1. When to use distributed processing\n",
    "2. Configuring distributed execution\n",
    "3. Running distributed processing pipelines\n",
    "4. Performance optimization techniques\n",
    "5. Other distributed processing methods (DLC, Slurm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "when-to-use",
   "metadata": {},
   "source": [
    "## When to Use Distributed Processing\n",
    "\n",
    "Distributed processing is beneficial in several scenarios:\n",
    "\n",
    "1. **Large datasets**: When dealing with millions or billions of records that would take days or weeks to process on a single machine\n",
    "2. **Compute-intensive operations**: Operations that involve deep learning models (image captioning, text generation, etc.)\n",
    "3. **Memory constraints**: When datasets are too large to fit in a single machine's memory\n",
    "4. **Time-sensitive tasks**: When you need to complete processing within a specific timeframe\n",
    "\n",
    "Data-Juicer's distributed processing is built on Ray, which makes it easy to scale across multiple nodes and fully utilize cluster resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ray-integration",
   "metadata": {},
   "source": [
    "### Engine-Agnostic Design\n",
    "\n",
    "For most implementations of Data-Juicer operators, the core processing functions are engine-agnostic. Interoperability is primarily managed in [RayDataset](https://github.com/modelscope/data-juicer/blob/main/data_juicer/core/ray_data.py) and [RayExecutor](https://github.com/modelscope/data-juicer/blob/main/data_juicer/core/executor/ray_executor.py), which are subclasses of the base DJDataset and BaseExecutor, respectively, and support both Ray [Tasks](https://docs.ray.io/en/latest/ray-core/tasks.html) and [Actors](https://docs.ray.io/en/latest/ray-core/actors.html).\n",
    "\n",
    "### Special Consideration for Deduplication Operators\n",
    "\n",
    "The exception is the deduplication operators, which are challenging to scale in standalone mode. We provides special distributed versions of these operators with names prefixed with `ray_`. These include:\n",
    "\n",
    "- `ray_bts_minhash_deduplicator`: A distributed implementation of Union-Find with load balancing\n",
    "- `ray_document_deduplicator`: Deduplicates samples at the document level using exact matching in Ray distributed mode\n",
    "- `ray_image_deduplicator`: Deduplicates samples at the document level using exact matching of images in Ray distributed mode\n",
    "- `ray_video_deduplicator`: Deduplicates samples at document-level using exact matching of videos in Ray distributed mode\n",
    "\n",
    "These specialized operators are designed to handle the unique challenges of distributed deduplication efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configuration",
   "metadata": {},
   "source": [
    "## Configuring Distributed Execution\n",
    "\n",
    "### Basic Configuration\n",
    "\n",
    "To enable distributed processing in Data-Juicer, set these parameters in your configuration file:\n",
    "\n",
    "```yaml\n",
    "# Specify Ray executor\n",
    "executor_type: ray\n",
    "\n",
    "# Optional: Specify Ray cluster address, default is \"auto\"\n",
    "ray_address: auto\n",
    "```\n",
    "\n",
    "Let's look at some example configuration files that are already available in Data-Juicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e073752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp asset\n",
    "%mkdir -p configs/demo\n",
    "%mkdir -p demos/data\n",
    "%cp ../configs/demo/dedup-ray-bts.yaml configs/demo/\n",
    "%cp ../configs/demo/dedup-ray-bts-gpu.yaml configs/demo/\n",
    "%cp ../demos/data/demo-dataset-deduplication.jsonl demos/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "examine-configs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the existing demo configuration files for distributed processing\n",
    "print(\"Basic distributed deduplication config (configs/demo/dedup-ray-bts.yaml):\")\n",
    "!cat configs/demo/dedup-ray-bts.yaml\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"GPU-accelerated distributed deduplication config (configs/demo/dedup-ray-bts-gpu.yaml):\")\n",
    "!cat configs/demo/dedup-ray-bts-gpu.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3149da0e",
   "metadata": {},
   "source": [
    "Let's create a custom distributed configuration based on the existing examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bbbd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile configs/custom_distributed_example.yaml\n",
    "\n",
    "# Custom distributed processing config example\n",
    "\n",
    "# Global parameters\n",
    "project_name: 'custom-distributed-example'\n",
    "dataset_path: './demos/data/demo-dataset-deduplication.jsonl'  # Using existing demo data\n",
    "np: 4  # Number of subprocesses to process your dataset\n",
    "\n",
    "# Distributed processing parameters\n",
    "executor_type: ray\n",
    "ray_address: auto\n",
    "open_monitor: true\n",
    "open_tracer: true\n",
    "\n",
    "# Output path\n",
    "export_path: './outputs/custom-distributed-example/processed.jsonl'\n",
    "\n",
    "# Process schedule\n",
    "process:\n",
    "  - language_id_score_filter:\n",
    "      lang: en\n",
    "      min_score: 0.5\n",
    "  - ray_bts_minhash_deduplicator:\n",
    "      tokenization: 'character'\n",
    "      lowercase: true\n",
    "      union_find_parallel_num: 2\n",
    "      # For GPU acceleration, add: accelerator: 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-distributed",
   "metadata": {},
   "source": [
    "## Running Distributed Processing Pipelines\n",
    "\n",
    "### Starting a Ray Cluster\n",
    "\n",
    "Before running distributed tasks, you need to start a Ray cluster:\n",
    "\n",
    "```bash\n",
    "# On the head node\n",
    "ray start --head --port=6379\n",
    "\n",
    "# On worker nodes (replace <HEAD_IP> with head node IP)\n",
    "ray start --address='<HEAD_IP>:6379'\n",
    "```\n",
    "\n",
    "### Running Distributed Processing Jobs\n",
    "\n",
    "Use the following commands to run distributed processing tasks with the existing configs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-cpu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CPU-based distributed deduplication\n",
    "# Note: This would normally be run with a real Ray cluster\n",
    "print(\"Command to run CPU-based distributed deduplication:\")\n",
    "!dj-process --config configs/custom_distributed_example.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-gpu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GPU-accelerated distributed deduplication\n",
    "# Note: This would normally be run with a real Ray cluster with GPUs\n",
    "# For demonstration purposes, we'll just show the command\n",
    "print(\"Command to run GPU-accelerated distributed deduplication:\")\n",
    "print(\"dj-process --config configs/demo/dedup-ray-bts-gpu.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-optimization",
   "metadata": {},
   "source": [
    "## Performance Optimization Techniques\n",
    "\n",
    "### Streaming Reading of JSON Files\n",
    "\n",
    "Streaming reading is crucial for processing large JSONL datasets without memory issues.\n",
    "\n",
    "Many datasets are stored in JSONL format and can be extremely large. The standard Ray Datasets implementation (up to Ray version 2.40 and Arrow version 18.1.0) doesn't support streaming reading of JSON files, leading to potential Out-of-Memory issues.\n",
    "\n",
    "Data-Juicer addresses this by:\n",
    "\n",
    "1. **Developing a streaming loading interface**\n",
    "2. **Contributing a patch to Apache Arrow** ([PR #45084](https://github.com/apache/arrow/pull/45084))\n",
    "3. **Enabling streaming-read support** for JSON, CSV, and Parquet files\n",
    "\n",
    "With this optimization, Data-Juicer in Ray mode uses streaming loading by default for JSON files, significantly reducing memory usage for large datasets.\n",
    "\n",
    "### Subset Splitting\n",
    "\n",
    "When dealing with many nodes but few dataset files, Ray's default behavior can be inefficient. \n",
    "\n",
    "Data-Juicer provides automatic dataset splitting to optimize performance: \n",
    "\n",
    "The single file size is set to 128MB, ensuring the number of sub-files is at least twice the total number of CPU cores in the cluster. The corresponding tool can be obtained in [tools/data_resplit.py]([tools/data_resplit.py](https://github.com/modelscope/data-juicer/blob/main/tools/data_resplit.py))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdf63b4",
   "metadata": {},
   "source": [
    "### Distributed Deduplication Optimizations\n",
    "\n",
    "Deduplication operations are particularly challenging to scale, which is why Data-Juicer provides specialized distributed versions of deduplication operators.\n",
    "\n",
    "Standard deduplication algorithms don't scale well in distributed environments due to:\n",
    "1. High memory requirements for hash tables\n",
    "2. Network communication overhead\n",
    "3. Load balancing issues\n",
    "\n",
    "For the `ray_bts_minhash_deduplicator`, Data-Juicer implements:\n",
    "\n",
    "1. Multiprocess Union-Find set in Ray Actors\n",
    "2. Load-balanced distributed algorithm (BTS) for equivalence class merging\n",
    "\n",
    "This optimization enables Data-Juicer to:\n",
    "- Deduplicate terabyte-sized datasets on 1280 CPU cores in 3 hours\n",
    "- Achieve 2x to 3x speedups compared to vanilla deduplication operators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-methods",
   "metadata": {},
   "source": [
    "## Other Distributed Processing Methods\n",
    "\n",
    "### DLC (Deep Learning Containers)\n",
    "\n",
    "Data-Juicer supports running distributed tasks in Alibaba Cloud PAI's DLC environment. Related scripts are in the `./scripts/dlc` directory:\n",
    "\n",
    "- [`partition_data_dlc.py`](https://github.com/modelscope/data-juicer/blob/main/scripts/dlc/partition_data_dlc.py): Partitions datasets across multiple nodes\n",
    "- [`run_on_dlc.sh`](https://github.com/modelscope/data-juicer/blob/main/scripts/dlc/run_on_dlc.sh): Script to run processing tasks in DLC environment\n",
    "\n",
    "### Slurm\n",
    "\n",
    "Data-Juicer also supports running distributed tasks on Slurm scheduling systems. Related scripts:\n",
    "\n",
    "- [`run_slurm.sh`](https://github.com/modelscope/data-juicer/blob/main/scripts/run_slurm.sh): Script to run distributed processing tasks on Slurm clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Continue with the next notebook to explore Data-Juicer's sandbox environment for data-model co-development."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-juicer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
