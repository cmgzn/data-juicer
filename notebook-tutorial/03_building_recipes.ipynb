{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "introduction",
      "metadata": {},
      "source": [
        "# Building Data Recipes\n",
        "\n",
        "In the previous notebooks, we learned the basic concepts of Data-Juicer and how to use operators. In this chapter, we'll dive deep into building data recipes, which is a crucial concept in Data-Juicer.\n",
        "\n",
        "## What are Data Recipes?\n",
        "\n",
        "Data recipes are configuration files written in YAML format that define a complete data processing workflow. Recipes combine various operators in a specific order to form an executable data processing pipeline.\n",
        "\n",
        "## In This Notebook\n",
        "\n",
        "1. Basic recipe structure\n",
        "2. Global parameters\n",
        "3. Process pipeline\n",
        "4. Recipe design best practices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, let's import the necessary modules and create some sample data to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Create sample data\n",
        "sample_data = [\n",
        "    {\"text\": \"Hello world! This is a sample text with good quality.\"},\n",
        "    {\"text\": \"This text has many repeated words words words words words words words words words words words words\"},\n",
        "    {\"text\": \"Short\"},\n",
        "    {\"text\": \"This is a high quality English text with appropriate length and good content.\"},\n",
        "    {\"text\": \"Bonjour le monde! Ceci est un texte d'exemple de bonne qualité.\"},\n",
        "    {\"text\": \"Visit https://example.com for more info. Email me at test@example.com\"},\n",
        "    {\"text\": \"This is a high quality English text with appropriate length and good content.\"}\n",
        "]\n",
        "\n",
        "# Create data directory if it doesn't exist\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Write sample data to a JSONL file\n",
        "with open('data/sample_dataset.jsonl', 'w') as f:\n",
        "    for item in sample_data:\n",
        "        f.write(json.dumps(item) + '\\n')\n",
        "\n",
        "print(\"Sample dataset created with\", len(sample_data), \"samples\")\n",
        "print(\"\\nOriginal samples:\")\n",
        "for i, sample in enumerate(sample_data):\n",
        "    print(f\"{i+1}. {sample['text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "basic-structure",
      "metadata": {},
      "source": [
        "## Basic Recipe Structure\n",
        "\n",
        "A typical Data-Juicer recipe contains the following parts:\n",
        "\n",
        "- **Global parameters**: Define project name, dataset path, export path, etc.\n",
        "- **Process pipeline**: Define the sequence of operators to execute and their parameters\n",
        "\n",
        "Let's start with a simple example to understand the basic structure of a recipe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "basic-recipe",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tempfile\n",
        "from data_juicer.config import init_configs\n",
        "\n",
        "basic_recipe = \"\"\"\n",
        "# Basic recipe example\n",
        "project_name: 'my_first_recipe'\n",
        "dataset_path: './data/sample_dataset.jsonl'\n",
        "export_path: './outputs/processed_dataset.jsonl'\n",
        "np: 4\n",
        "\n",
        "process:\n",
        "  - whitespace_normalization_mapper: {}\n",
        "  - language_id_score_filter:\n",
        "      lang: 'en'\n",
        "      min_score: 0.8\n",
        "  - text_length_filter:\n",
        "      min_len: 10\n",
        "      max_len: 1000\n",
        "\"\"\"\n",
        "\n",
        "# Write recipe to file\n",
        "with tempfile.NamedTemporaryFile(mode='w+', delete=False) as basic:\n",
        "    basic.write(basic_recipe)\n",
        "    basic.flush()\n",
        "\n",
        "# Load and check the recipe\n",
        "cfg = init_configs(args=f'--config {basic.name}'.split())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dataset-configuration",
      "metadata": {},
      "source": [
        "## Global Configuration Parameters\n",
        "\n",
        "Global parameters control the overall behavior of the data processing pipeline. These are divided into several categories:\n",
        "\n",
        "Note: While we demonstrate the detailed operations using tools like `DatasetBuilder` and `init_configs` for educational purposes, in practice these operations are all handled automatically by Data-Juicer's dj-process command. You can also use `DefaultExecutor` or `RayExecutor`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Dataset Configuration\n",
        "\n",
        "Dataset configuration defines where to load data from and how to handle it:\n",
        "\n",
        "#### Simple Dataset Configuration\n",
        "\n",
        "For basic use cases, you can use the `dataset_path` parameter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "simple-dataset-config",
      "metadata": {},
      "outputs": [],
      "source": [
        "from data_juicer.core.data.dataset_builder import DatasetBuilder\n",
        "\n",
        "simple_recipe = \"\"\"\n",
        "# Simple dataset configuration\n",
        "project_name: 'simple_dataset_config'\n",
        "dataset_path: './data/sample_dataset.jsonl'\n",
        "\"\"\"\n",
        "\n",
        "# Write recipe to file\n",
        "with tempfile.NamedTemporaryFile(mode='w+', delete=False) as simple:\n",
        "    simple.write(simple_recipe)\n",
        "    simple.flush()\n",
        "\n",
        "# Load and check the recipe\n",
        "cfg = init_configs(args=f'--config {simple.name}'.split(), load_configs_only=True)\n",
        "\n",
        "# Use the DatasetBuilder to load the dataset\n",
        "dataset_builder = DatasetBuilder(cfg)\n",
        "dataset = dataset_builder.load_dataset()\n",
        "\n",
        "print(dataset.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "advanced-dataset-config",
      "metadata": {},
      "source": [
        "#### Advanced Dataset Configuration\n",
        "\n",
        "For more complex scenarios, Data-Juicer provides flexible dataset loading methods. This approach allows you to:\n",
        "\n",
        "1. Load different types of datasets (local, remote, HuggingFace, etc.)\n",
        "2. Mix multiple datasets with different weights\n",
        "3. Apply data validation rules\n",
        "4. Configure advanced loading parameters\n",
        "\n",
        "Here's how to use the advanced dataset configuration:\n",
        "\n",
        "##### Local Datasets\n",
        "\n",
        "You can load local datasets in various formats:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "local-dataset-config",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Formats like parquet, jsonl, json, csv, tsv, txt, and jsonl.gz are supported\n",
        "local_json_recipe = \"\"\"\n",
        "# Loading a local JSON dataset\n",
        "project_name: 'local_json_dataset'\n",
        "\n",
        "dataset:\n",
        "  configs:\n",
        "    - type: 'local'\n",
        "      path: './data/sample_dataset.jsonl'\n",
        "      format: 'json'\n",
        "\n",
        "# Optional data validators\n",
        "validators:\n",
        "  - type: required_fields\n",
        "    required_fields:\n",
        "      - \"text\"\n",
        "\"\"\"\n",
        "\n",
        "with tempfile.NamedTemporaryFile(mode='w+', delete=False) as local_json:\n",
        "    local_json.write(local_json_recipe)\n",
        "    local_json.flush()\n",
        "\n",
        "cfg = init_configs(args=f'--config {local_json.name}'.split(), load_configs_only=True)\n",
        "\n",
        "dataset_builder = DatasetBuilder(cfg)\n",
        "dataset = dataset_builder.load_dataset()\n",
        "\n",
        "print(dataset.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "remote-datasets",
      "metadata": {},
      "source": [
        "##### Remote Datasets\n",
        "\n",
        "You can also load datasets from remote sources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "remote-dataset-config",
      "metadata": {},
      "outputs": [],
      "source": [
        "remote_hf_recipe = \"\"\"\n",
        "# Loading a HuggingFace dataset (example)\n",
        "project_name: 'remote_hf_dataset'\n",
        "dataset:\n",
        "  configs:\n",
        "    - type: 'remote'\n",
        "      source: 'huggingface'\n",
        "      path: \"wikimedia/wikipedia\"\n",
        "      name: \"20231101.kl\"\n",
        "      split: \"train\"\n",
        "\"\"\"\n",
        "\n",
        "with tempfile.NamedTemporaryFile(mode='w+', delete=False) as remote_hf:\n",
        "    remote_hf.write(remote_hf_recipe)\n",
        "    remote_hf.flush()\n",
        "\n",
        "cfg = init_configs(args=f'--config {remote_hf.name}'.split(), load_configs_only=True)\n",
        "\n",
        "dataset_builder = DatasetBuilder(cfg)\n",
        "dataset = dataset_builder.load_dataset()\n",
        "\n",
        "print(dataset.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dataset-mixtures",
      "metadata": {},
      "source": [
        "##### Dataset Mixtures\n",
        "\n",
        "You can mix multiple datasets with different weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_data_zh = [\n",
        "    {\"text\": \"你好世界！这是一个样本文本。\"},\n",
        "    {\"text\": \"这是一段重复的文本文本文本文本文本文本文本文本文本文本\"},\n",
        "    {\"text\": \"短文本\"},\n",
        "    {\"text\": \"欢迎来到阿里巴巴！\"}\n",
        "]\n",
        "\n",
        "with open('data/sample_dataset_zh.jsonl', 'w') as f:\n",
        "    for item in sample_data_zh:\n",
        "        f.write(json.dumps(item) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mixture-dataset-config",
      "metadata": {},
      "outputs": [],
      "source": [
        "mixture_recipe = \"\"\"\n",
        "# Mixing multiple datasets\n",
        "project_name: 'dataset_mixture'\n",
        "dataset:\n",
        "  max_sample_num: 10\n",
        "  configs:\n",
        "    - type: 'local'\n",
        "      weight: 1.0\n",
        "      path: './data/sample_dataset.jsonl'\n",
        "    - type: 'local'\n",
        "      weight: 0.5\n",
        "      path: './data/sample_dataset_zh.jsonl'\n",
        "\"\"\"\n",
        "\n",
        "with tempfile.NamedTemporaryFile(mode='w+', delete=False) as mixture:\n",
        "    mixture.write(mixture_recipe)\n",
        "    mixture.flush()\n",
        "\n",
        "cfg = init_configs(args=f'--config {mixture.name}'.split(), load_configs_only=True)\n",
        "dataset_builder = DatasetBuilder(cfg)\n",
        "dataset = dataset_builder.load_dataset()\n",
        "\n",
        "for i, sample in enumerate(dataset):\n",
        "    print(f\"{i+1}. {sample['text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Export Configuration\n",
        "\n",
        "Controls how and where to save the processed data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import tempfile\n",
        "from data_juicer.config import init_configs\n",
        "from data_juicer.core.exporter import Exporter\n",
        "\n",
        "export_config_recipe = \"\"\"\n",
        "# Export configuration example\n",
        "project_name: 'export_config_example'\n",
        "dataset_path: './data/sample_dataset.jsonl'\n",
        "\n",
        "# Export settings\n",
        "export_path: './outputs/exported_data.json'\n",
        "export_type: 'json'  # or 'parquet', 'jsonl', etc. Optional.\n",
        "export_shard_size: 0  # 0 means no sharding. Optional.\n",
        "export_in_parallel: false  # Whether to export in parallel. Optional.\n",
        "\"\"\"\n",
        "\n",
        "with tempfile.NamedTemporaryFile(mode=\"w+\", delete=False) as export_cfg:\n",
        "    export_cfg.write(export_config_recipe)\n",
        "    export_cfg.flush()\n",
        "\n",
        "cfg = init_configs(args=f\"--config {export_cfg.name}\".split(), load_configs_only=True)\n",
        "\n",
        "dataset = DatasetBuilder(cfg).load_dataset()\n",
        "# Let's export the dataset loaded in the previous step to JSON format\n",
        "# This is merely a demonstration\n",
        "# In executor, the Exporter will be used to export the final results of data processing\n",
        "exporter = Exporter(cfg.export_path, cfg.export_type, cfg.export_shard_size, cfg.export_in_parallel)\n",
        "exporter.export(dataset)\n",
        "\n",
        "if os.path.exists(cfg.export_path):\n",
        "    print(f\"Exported data saved to {cfg.export_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. System and Runtime Configuration\n",
        "\n",
        "Controls system-level behavior and resource usage:\n",
        "\n",
        "Note: The following parameters are for illustration only. For a complete list of available options, please refer to [`config_all.yaml`](https://github.com/modelscope/data-juicer/blob/main/configs/config_all.yaml)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tempfile\n",
        "from data_juicer.config import init_configs\n",
        "\n",
        "system_config_recipe = \"\"\"\n",
        "# System configuration example\n",
        "project_name: 'system_config_example'\n",
        "dataset_path: './data/sample_dataset.jsonl'\n",
        "export_path: './outputs/system_result.jsonl'\n",
        "\n",
        "# Runtime settings\n",
        "np: 4  # Number of processes\n",
        "text_keys: 'text'  # Default text field name\n",
        "image_key: 'images'  # Default image field name\n",
        "\n",
        "# Caching and performance\n",
        "use_cache: true\n",
        "cache_compress: 'gzip'\n",
        "ds_cache_dir: '~/.cache/huggingface/datasets'  # Dataset cache directory\n",
        "\n",
        "# Monitoring and debugging\n",
        "open_monitor: true  # Enable system monitoring\n",
        "open_tracer: false  # Enable operation tracing\n",
        "debug: false  # Debug mode\n",
        "\n",
        "# Checkpointing for long-running jobs\n",
        "use_checkpoint: false\n",
        "\n",
        "process:\n",
        "  - whitespace_normalization_mapper: {}\n",
        "\"\"\"\n",
        "\n",
        "with tempfile.NamedTemporaryFile(mode='w+', delete=False) as system_cfg:\n",
        "    system_cfg.write(system_config_recipe)\n",
        "    system_cfg.flush()\n",
        "\n",
        "cfg = init_configs(args=f'--config {system_cfg.name}'.split(), load_configs_only=True)\n",
        "print(\"System configuration loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process Pipeline Configuration\n",
        "\n",
        "The `process` section defines the sequence of operators to be applied to the dataset. Each operator has its own parameter configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Operator Configuration Syntax\n",
        "\n",
        "Operators are listed in order in the `process` list, with each operator's configuration following this syntax:\n",
        "\n",
        "```yaml\n",
        "process:\n",
        "  - operator_name:\n",
        "      parameter1: value1\n",
        "      parameter2: value2\n",
        "      ...\n",
        "```\n",
        "\n",
        "If there are no parameters, it can be simplified to:\n",
        "\n",
        "```yaml\n",
        "process:\n",
        "  - operator_name: {}\n",
        "```\n",
        "\n",
        "### Example: Complete Process Pipeline\n",
        "\n",
        "Let's look at a more complete example showing how to configure a process pipeline with multiple operators:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from data_juicer.ops import load_ops\n",
        "\n",
        "complete_recipe = \"\"\"\n",
        "# Complete recipe example\n",
        "project_name: 'complete_recipe_example'\n",
        "\n",
        "# Text field configuration\n",
        "text_keys: 'text'\n",
        "\n",
        "# Process pipeline\n",
        "process:\n",
        "  # 1. Text cleaning and normalization\n",
        "  - whitespace_normalization_mapper: {}\n",
        "  - punctuation_normalization_mapper: {}\n",
        "  \n",
        "  # 2. Language detection and filtering\n",
        "  - language_id_score_filter:\n",
        "      lang: 'en'\n",
        "      min_score: 0.8\n",
        "  \n",
        "  # 3. Text length filtering\n",
        "  - text_length_filter:\n",
        "      min_len: 10\n",
        "      max_len: 1000\n",
        "  \n",
        "  # 4. Quality filtering\n",
        "  - alphanumeric_filter:\n",
        "      tokenization: false\n",
        "      min_ratio: 0.5\n",
        "  - average_line_length_filter:\n",
        "      min_len: 10\n",
        "      max_len: 1000\n",
        "  \n",
        "  # 5. Deduplication\n",
        "  - document_simhash_deduplicator:\n",
        "      tokenization: false\n",
        "      window_size: 6\n",
        "      lower: true\n",
        "      ignore_pattern: null\n",
        "      num_blocks: 6\n",
        "      hamming_distance: 4\n",
        "\"\"\"\n",
        "\n",
        "with tempfile.NamedTemporaryFile(mode='w+', delete=False) as complete:\n",
        "    complete.write(complete_recipe)\n",
        "    complete.flush()\n",
        "\n",
        "cfg = init_configs(args=f'--config {complete.name}'.split())\n",
        "\n",
        "# Load operators (this is what happens internally)\n",
        "ops = load_ops(cfg.process)\n",
        "print(f\"Loaded {len(ops)} operators:\")\n",
        "for i, op in enumerate(ops):\n",
        "    print(f\"  {i+1}. {op.__class__.__name__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices for Recipe Design\n",
        "\n",
        "There are three approaches to constructing a data recipe.\n",
        "\n",
        "### Customize the Default Configuration File\n",
        "\n",
        "The [`config_all.yaml`](https://github.com/modelscope/data-juicer/blob/main/configs/config_all.yaml) contains all operators and their default arguments. \n",
        "\n",
        "You just need to **remove** ops that you won't use and refine some arguments of ops.\n",
        "\n",
        "### Create a New Configuration from Scratch\n",
        "\n",
        "You can refer our example config file [`config_all.yaml`](https://github.com/modelscope/data-juicer/blob/main/configs/config_all.yaml), [op documents](https://modelscope.github.io/data-juicer/en/main/docs/Operators.html), and [Dataset Configuration Guide](https://modelscope.github.io/data-juicer/en/main/docs/DatasetCfg.html).\n",
        "\n",
        "### Reusable Recipes\n",
        "\n",
        "Data-Juicer provides a rich collection of reusable data recipes in the [Recipe Gallery](https://modelscope.github.io/data-juicer/en/main/docs/RecipeGallery.html). These recipes cover various scenarios including:\n",
        "\n",
        "1. **Minimal Example Recipes**: Basic configurations to get you started\n",
        "2. **Reproducing Open Source Datasets**: Recipes that reproduce the processing pipelines of popular datasets like RedPajama and BLOOM\n",
        "3. **Refined Pre-training Datasets**: Improved versions of existing pre-training datasets with better quality\n",
        "4. **Post-tuning Dataset Improvements**: Refined instruction datasets for fine-tuning\n",
        "5. **Multimodal Dataset Processing**: Recipes for image-text and video datasets\n",
        "6. **Synthetic Dataset Generation**: Recipes for generating contrastive learning datasets\n",
        "\n",
        "We recommend exploring the [Recipe Gallery](https://modelscope.github.io/data-juicer/en/main/docs/RecipeGallery.html) to find recipes that match your use case, which can serve as a starting point for your own data processing workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "Continue with the next notebook in the series to learn how to actually run these recipes to process datasets."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "data-juicer",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
